goals:
- scrape arxiv
  - they don't like scraping
  - only way, unless I want *all* of the articles, then I can get them using S3
  - use API for search: http://arxiv.org/help/api/index
    - okay, don't use API for search. API doesn't do full-text search.
  - scraping is done, missing 8 papers. I think those are duplicates
    - they seem to be duplicates. http://arxiv.org/pdf/cs.LG/0212028 vs http://arxiv.org/pdf/cs/0212028 (same paper)
- make corpus
  - arxiv is mostly pdf, contains some ps and text and source sometimes
    - don't use source, that would need tex parser (this might be do-able)
    - it turns out pdftotext is really good at extracting text from pdfs
      - the layout option is also nice, but not really for this purpose
  - tokenization
    - use http://www.statmt.org/moses/?n=moses.baseline
    - keep named entities, dataset isn't likely to have a lot except for references
    - subsampling as in Mikholov.c?
    - filter clusters(>1) of lines less than 30 characters long (except the first one coming after a long line)
- distributed vector representation
  - use skipgrams
- use PCA


task:
If you are wondering why we put up a task, read "Delta Force" by Charlie Beckwith (management perspective) and "Inside Delta Force" by Eric Haney (data scientist perspective). 

Here are 453 papers (as of this writing) containing recent IT bubble words like "big data": http://tinyurl.com/mczfmz9
Here is a paper: http://arxiv.org/abs/1309.4168

Crawl the first link to download all the papers, make them into a corpus, apply the methods in the second link, use PCA to reduce it to two dimensions, plot the words in question, send us the code for everything, the corpus, and include the graph in the body of the email. Try to write as much as possible in Haskell.